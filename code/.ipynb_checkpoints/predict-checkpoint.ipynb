{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "Make sure to use theano backend for this\n",
    "- Modify your ~/.keras/keras.json to change the backend to \"theano\"\n",
    "- Set variables FN0, FN1, nb_train_samples, nb_val_samples and rnn_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = 'predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if your GPU is busy you can use CPU for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'device=cpu,floatX=float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate headlines using the \"simple\" model from http://arxiv.org/pdf/1512.01712v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use indexing of tokens from [vocabulary-embedding](./vocabulary-embedding.ipynb) this does not clip the indexes of the words to `vocab_size`.\n",
    "\n",
    "Use the index of outside words to replace them with several `oov` words (`oov` , `oov0`, `oov`...) that appear in the same description and headline. This will allow headline generator to replace the oov with the same word in the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2758,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN0 = 'json_news-5000-glove-vocab-embedding'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will generate predictions using the model generated in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2759,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN1 = 'train-5000-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2760,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = 5000\n",
    "nb_val_samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input data (`X`) is made from `maxlend` description words followed by `eos`\n",
    "followed by headline words followed by `eos`\n",
    "if description is shorter than `maxlend` it will be left padded with `empty`\n",
    "if entire data is longer than `maxlen` it will be clipped and if it is shorter it will be padded.\n",
    "\n",
    "labels (`Y`) are the headline words followed by `eos` and clipped or padded to `maxlenh`\n",
    "\n",
    "In other words the input is made from a `maxlend` half in which the description is padded from the left\n",
    "and a `maxlenh` half in which `eos` is followed by a headline followed by another `eos` if there is enough space.\n",
    "\n",
    "The labels match only the second half and \n",
    "the first label matches the `eos` at the start of the second half (following the description in the first half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model parameters should be identical with what used in training but notice that `maxlend` is flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2761,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlend = 25 # 0 - if we dont want to use description at all\n",
    "maxlenh = 25\n",
    "maxlen = maxlend + maxlenh\n",
    "rnn_size = 512 * 2 if 'bilstm' in FN1 else 512\n",
    "rnn_layers = 2  # match FN1\n",
    "# rnn_layers = 3\n",
    "batch_norm = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the out of the first `activation_rnn_size` nodes from the top layer will be used for activation and the rest will be used to select predicted word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2762,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_rnn_size = 40 if maxlend else 0\n",
    "\n",
    "lambda_dim = 2*(rnn_size - activation_rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2763,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "seed = 42\n",
    "p_W, p_U, p_dense, weight_decay = 0, 0, 0, 0\n",
    "optimizer = 'rmsprop'\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2764,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/%s.pickle'%FN0, 'rb') as fp:\n",
    "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
    "vocab_size, embedding_size = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2765,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_unknown_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of embedding space for words 100\n",
      "vocabulary size 17493 the last 10 words can be used as place holders for unknown/oov words\n",
      "total number of different words 37514 37514\n",
      "number of words outside vocabulary which we can substitue using glove similarity 8382\n",
      "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov) 11639\n"
     ]
    }
   ],
   "source": [
    "print('dimension of embedding space for words',embedding_size)\n",
    "print('vocabulary size', vocab_size, 'the last %d words can be used as place holders for unknown/oov words'%nb_unknown_words)\n",
    "print('total number of different words',len(idx2word), len(word2idx))\n",
    "print('number of words outside vocabulary which we can substitue using glove similarity', len(glove_idx2idx))\n",
    "print('number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov)',len(idx2word)-vocab_size-len(glove_idx2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2767,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size-1-i] = '<%d>'%i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2768,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(vocab_size-nb_unknown_words, len(idx2word)):\n",
    "    idx2word[i] = idx2word[i]+'^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2769,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = 0\n",
    "eos = 1\n",
    "idx2word[empty] = '_'\n",
    "idx2word[eos] = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2770,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import random, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2771,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prt(label, x):\n",
    "    print(label+':'),\n",
    "    for w in x:\n",
    "        print(idx2word[w]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theano'"
      ]
     },
     "execution_count": 2772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n",
    "K.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2773,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed weight initialization\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2774,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = l2(weight_decay) if weight_decay else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a stacked LSTM, which is identical to the bottom of the model used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2775,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential()\n",
    "if DEBUG:\n",
    "    print('DEBUG')\n",
    "    from keras.layers import InputLayer\n",
    "    model_input = InputLayer(input_shape=(maxlen,))\n",
    "    rnn_model.add(model_input)\n",
    "    rnn_model.output.tag.test_value = np.random.randint(vocab_size,size=(batch_size,maxlen)).astype('float32')\n",
    "rnn_model.add(Embedding(vocab_size, embedding_size,\n",
    "                        input_length=maxlen,\n",
    "#                         batch_input_shape=(batch_size,maxlen),\n",
    "                        embeddings_regularizer=regularizer, weights=[embedding], mask_zero=True,\n",
    "                        name='embedding_1'))\n",
    "for i in range(rnn_layers):\n",
    "    lstm = LSTM(rnn_size, return_sequences=True, # batch_norm=batch_norm,\n",
    "                kernel_regularizer=regularizer, recurrent_regularizer=regularizer,\n",
    "                bias_regularizer=regularizer, dropout=p_W, recurrent_dropout=p_U,\n",
    "                name='lstm_%d'%(i+1)\n",
    "                  )\n",
    "    rnn_model.add(lstm)\n",
    "    rnn_model.add(Dropout(p_dense, name='dropout_%d'%(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2776,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(rnn_model.output.tag.test_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the bottom weights from the trained model, and save the top weights for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           1749300   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 512)           1255424   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 512)           0         \n",
      "=================================================================\n",
      "Total params: 5,103,924\n",
      "Trainable params: 5,103,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.load_weights('data/%s.hdf5'%FN1,by_name=True)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.01527273,  0.05486724,  0.02824154, ..., -0.00882072,\n",
      "        -0.05777261, -0.04773293],\n",
      "       [-0.05824773,  0.01578965, -0.01994223, ...,  0.04531222,\n",
      "         0.04875275,  0.03054144],\n",
      "       [-0.00444155, -0.0264328 ,  0.07507543, ..., -0.01620985,\n",
      "         0.08389987,  0.0247697 ],\n",
      "       ...,\n",
      "       [-0.0364346 ,  0.03310443,  0.01621359, ...,  0.00080122,\n",
      "        -0.05824996, -0.01143235],\n",
      "       [ 0.01956535, -0.02479992,  0.00477357, ..., -0.00473689,\n",
      "         0.02420149,  0.08577059],\n",
      "       [-0.00399871, -0.05687615,  0.06186307, ...,  0.01086607,\n",
      "        -0.00127954,  0.02430375]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(rnn_model.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2780,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('data/%s.hdf5'%FN1, mode='r') as f:\n",
    "    if 'layer_names' not in f.attrs and 'model_weights' in f:\n",
    "        f = f['model_weights']\n",
    "    weights = [np.copy(v) for v in f['time_distributed_2']['time_distributed_2'].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(944, 17493), (17493,)]"
      ]
     },
     "execution_count": 2781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kernel, bias\n",
    "weights = np.array([weights[1], weights[0]])\n",
    "list(map(lambda x: x.shape, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## headline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special layer that reduces the input just to its headline part (second half).\n",
    "For each word in this part it concatenate the output of the previous layer (RNN)\n",
    "with a weighted average of the outputs of the description part.\n",
    "In this only the last `rnn_size - activation_rnn_size` are used from each output.\n",
    "The first `activation_rnn_size` output is used to computer the weights for the averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2782,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_context(X, mask, n=activation_rnn_size, maxlend=maxlend, maxlenh=maxlenh):\n",
    "    desc, head = X[:,:maxlend,:], X[:,maxlend:,:]\n",
    "    head_activations, head_words = head[:,:,:n], head[:,:,n:]\n",
    "    desc_activations, desc_words = desc[:,:,:n], desc[:,:,n:]\n",
    "    \n",
    "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
    "    # activation for every head word and every desc word\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=([2],[2]))\n",
    "    # make sure we dont use description words that are masked out\n",
    "    activation_energies = activation_energies + -1e20*K.expand_dims(1.-K.cast(mask[:, :maxlend],'float32'),1)\n",
    "    \n",
    "    # for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights,(-1,maxlenh,maxlend))\n",
    "\n",
    "    # for every head word compute weighted average of desc words\n",
    "    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=([2],[1]))\n",
    "    if DEBUG:\n",
    "        print(desc_avg_word.tag.test_value.shape)\n",
    "        print(head_words.tag.test_value.shape)\n",
    "    return K.concatenate((desc_avg_word, head_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2783,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_111 (Sequential)  (None, 50, 512)           5103924   \n",
      "=================================================================\n",
      "Total params: 5,103,924\n",
      "Trainable params: 5,103,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(rnn_model)\n",
    "model.summary()\n",
    "if activation_rnn_size:\n",
    "    model.add(Lambda(simple_context,\n",
    "                     mask = lambda inputs, mask: mask[:,maxlend:],\n",
    "                     output_shape = lambda input_shape: (input_shape[0], maxlenh, lambda_dim),\n",
    "                     name='simplecontext_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2784,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are not going to fit so we dont care about loss and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "944"
      ]
     },
     "execution_count": 2785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = lambda_dim\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform the top dense of the trained model in numpy so we can play around with exactly how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2786,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out very own softmax\n",
    "def output2probs(output):\n",
    "    output = np.dot(output, weights[0]) + weights[1]\n",
    "    output -= output.max()\n",
    "    output = np.exp(output)\n",
    "    output /= output.sum()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2787,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output2probs1(output):\n",
    "    output0 = np.dot(output[:n//2], weights[0][:n//2,:])\n",
    "    output1 = np.dot(output[n//2:], weights[0][n//2:,:])\n",
    "    output = output0 + output1 # + output0 * output1\n",
    "    output += weights[1]\n",
    "    output -= output.max()\n",
    "    output = np.exp(output)\n",
    "    output /= output.sum()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2788,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpadd(x, maxlend=maxlend, eos=eos):\n",
    "    \"\"\"left (pre) pad a description to maxlend and then add eos.\n",
    "    The eos is the input to predicting the first word in the headline\n",
    "    \"\"\"\n",
    "    assert maxlend >= 0\n",
    "    if maxlend == 0:\n",
    "        return [eos]\n",
    "    n = len(x)\n",
    "    if n > maxlend:\n",
    "        x = x[-maxlend:]\n",
    "        n = maxlend\n",
    "    return [empty]*(maxlend-n) + x + [eos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1]]\n",
      "[[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "samples = [lpadd([3]*26)]\n",
    "# pad from right (post) so the first maxlend will be description followed by headline\n",
    "print(samples)\n",
    "data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(data[:,maxlend] == eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 50), [26])"
      ]
     },
     "execution_count": 2791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,list(map(len, samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25, 944)"
      ]
     },
     "execution_count": 2792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(data, verbose=0, batch_size=1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2793,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variation to https://github.com/ryankiros/skip-thoughts/blob/master/decoding/search.py\n",
    "def beamsearch(predict, start=[empty]*maxlend + [eos], avoid=None, avoid_score=1,\n",
    "               k=1, maxsample=maxlen, use_unk=True, oov=vocab_size-1, empty=empty, eos=eos, temperature=1.0):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    def sample(energy, n, temperature=temperature):\n",
    "        \"\"\"sample at most n different elements according to their energy\"\"\"\n",
    "        n = min(n,len(energy))\n",
    "        prb = np.exp(-np.array(energy) / temperature )\n",
    "        res = []\n",
    "        for i in range(n):\n",
    "            z = np.sum(prb)\n",
    "            r = np.argmax(np.random.multinomial(1, prb/z, 1))\n",
    "            res.append(r)\n",
    "            prb[r] = 0. # make sure we select each element only once\n",
    "        return res\n",
    "\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_samples = [list(start)]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_samples:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    "        assert vocab_size == probs.shape[1]\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        cand_scores[:,empty] = 1e20\n",
    "        if not use_unk and oov is not None:\n",
    "            cand_scores[:,oov] = 1e20\n",
    "        if avoid:\n",
    "            for a in avoid:\n",
    "                for i, s in enumerate(live_samples):\n",
    "                    n = len(s) - len(start)\n",
    "                    if n < len(a):\n",
    "                        # at this point live_sample is before the new word,\n",
    "                        # which should be avoided, is added\n",
    "                        cand_scores[i,a[n]] += avoid_score\n",
    "        live_scores = list(cand_scores.flatten())\n",
    "        \n",
    "\n",
    "        # find the best (lowest) scores we have from all possible dead samples and\n",
    "        # all live samples and all possible new words added\n",
    "        scores = dead_scores + live_scores\n",
    "        ranks = sample(scores, k)\n",
    "        n = len(dead_scores)\n",
    "        dead_scores = [dead_scores[r] for r in ranks if r < n]\n",
    "        dead_samples = [dead_samples[r] for r in ranks if r < n]\n",
    "        \n",
    "        live_scores = [live_scores[r-n] for r in ranks if r >= n]\n",
    "        live_samples = [live_samples[(r-n)//vocab_size]+[(r-n)%vocab_size] for r in ranks if r >= n]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        # even if len(live_samples) == maxsample we dont want it dead because we want one\n",
    "        # last prediction out of it to reach a headline of maxlenh\n",
    "        def is_zombie(s):\n",
    "            return s[-1] == eos or len(s) > maxsample\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_scores += [c for s, c in zip(live_samples, live_scores) if is_zombie(s)]\n",
    "        dead_samples += [s for s in live_samples if is_zombie(s)]\n",
    "        \n",
    "        # remove zombies from the living \n",
    "        live_scores = [c for s, c in zip(live_samples, live_scores) if not is_zombie(s)]\n",
    "        live_samples = [s for s in live_samples if not is_zombie(s)]\n",
    "\n",
    "    return dead_samples, dead_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2794,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2795,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_rnn_predict(samples, empty=empty, model=model, maxlen=maxlen):\n",
    "    \"\"\"for every sample, calculate probability for every possible label\n",
    "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
    "    \"\"\"\n",
    "    sample_lengths = list(map(len, samples))\n",
    "    assert all(l > maxlend for l in sample_lengths)\n",
    "    assert all(l[maxlend] == eos for l in samples)\n",
    "    # pad from right (post) so the first maxlend will be description followed by headline\n",
    "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "    probs = model.predict(data, verbose=0, batch_size=batch_size)\n",
    "    return np.array([output2probs(prob[sample_length-maxlend-1]) for prob, sample_length in zip(probs, sample_lengths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2796,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_fold(xs):\n",
    "    \"\"\"convert list of word indexes that may contain words outside vocab_size to words inside.\n",
    "    If a word is outside, try first to use glove_idx2idx to find a similar word inside.\n",
    "    If none exist then replace all accurancies of the same unknown word with <0>, <1>, ...\n",
    "    \"\"\"\n",
    "    xs = [x if x < vocab_size-nb_unknown_words else glove_idx2idx.get(x,x) for x in xs]\n",
    "    # the more popular word is <0> and so on\n",
    "    outside = sorted([x for x in xs if x >= vocab_size-nb_unknown_words])\n",
    "    # if there are more than nb_unknown_words oov words then put them all in nb_unknown_words-1\n",
    "    outside = dict((x,vocab_size-1-min(i, nb_unknown_words-1)) for i, x in enumerate(outside))\n",
    "    xs = [outside.get(x,x) for x in xs]\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2797,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_unfold(desc,xs):\n",
    "    # assume desc is the unfolded version of the start of xs\n",
    "    unfold = {}\n",
    "    for i, unfold_idx in enumerate(desc):\n",
    "        fold_idx = xs[i]\n",
    "        if fold_idx >= vocab_size-nb_unknown_words:\n",
    "            unfold[fold_idx] = unfold_idx\n",
    "    return [unfold.get(x,x) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2798,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import Levenshtein\n",
    "\n",
    "def gensamples(X=None, X_test=None, Y_test=None, avoid=None, avoid_score=1, skips=2, k=10, batch_size=batch_size, short=True, temperature=1., use_unk=True):\n",
    "    if X is None or isinstance(X,int):\n",
    "        if X is None:\n",
    "            i = random.randint(0,len(X_test)-1)\n",
    "        else:\n",
    "            i = X\n",
    "        print('HEAD %d:'%i,' '.join(idx2word[w] for w in Y_test[i]))\n",
    "        print('DESC:',' '.join(idx2word[w] for w in X_test[i]))\n",
    "        sys.stdout.flush()\n",
    "        x = X_test[i]\n",
    "    else:\n",
    "        for w in X.split():\n",
    "            w = w.rstrip('^')\n",
    "            if not w in word2idx:\n",
    "                word2idx[w] = word2idx.get(w, len(word2idx))\n",
    "\n",
    "        x = [word2idx[w.rstrip('^')] for w in X.split()]\n",
    "        \n",
    "    if avoid:\n",
    "        # avoid is a list of avoids. Each avoid is a string or list of word indicies\n",
    "        if isinstance(avoid,str) or isinstance(avoid[0], int) or isinstance(avoid[0], np.int64):\n",
    "            avoid = [avoid]\n",
    "        avoid = [a.split() if isinstance(a,str) else a for a in avoid]\n",
    "        avoid = [vocab_fold([w if isinstance(w,int) or isinstance(w,np.int64) else word2idx[w] for w in a])\n",
    "                 for a in avoid]\n",
    "\n",
    "    print('HEADS:')\n",
    "    samples = []\n",
    "    if maxlend == 0:\n",
    "        skips = [0]\n",
    "    else:\n",
    "        skips = range(min(maxlend,len(x)), max(maxlend,len(x)), abs(maxlend - len(x)) // skips + 1)\n",
    "    for s in skips:\n",
    "        start = lpadd(x[:s])\n",
    "        fold_start = vocab_fold(start)\n",
    "        sample, score = beamsearch(predict=keras_rnn_predict, start=fold_start, avoid=avoid, avoid_score=avoid_score,\n",
    "                                   k=k, temperature=temperature, use_unk=use_unk)\n",
    "        assert all(s[maxlend] == eos for s in sample)\n",
    "        samples += [(s,start,scr) for s,scr in zip(sample,score)]\n",
    "\n",
    "    samples.sort(key=lambda x: x[-1])\n",
    "    codes = []\n",
    "    for sample, start, score in samples:\n",
    "        code = ''\n",
    "        words = []\n",
    "        sample = vocab_unfold(start, sample)[len(start):]\n",
    "        for w in sample:\n",
    "            if w == eos:\n",
    "                break\n",
    "            words.append(idx2word[w])\n",
    "            code += chr(w//(256*256)) + chr((w//256)%256) + chr(w%256)\n",
    "        if short:\n",
    "            distance = min([100] + [-Levenshtein.jaro(code,c) for c in codes])\n",
    "            if distance > -0.6:\n",
    "                print(score, ' '.join(words))\n",
    "        #         print '%s (%.2f) %f'%(' '.join(words), score, distance)\n",
    "        else:\n",
    "                print(score, ' '.join(words))\n",
    "        codes.append(code)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2799,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2800,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = \"* Billy Joel is looking for a buyer in Sagaponack^ . Now that he and wife Katie Lee Joel are splitting up , the singer is planning to sell the two oceanfront^ properties he bought for her in 2007 . The four-bedroom mansion ( No . 1 ) and smaller beach bungalow^ ( No . 2 ) will be listed with Corcoran 's Biana^ Stepanian^ for a combined $ 35 million . * Richard Bressler^ , the former CFO of Viacom and now a managing\"\n",
    "# Y = \"Billy Joel Lists in Sagaponack^\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2801,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD:  Patriots Day Is Best When It Digs Past the Heroism\n",
      "HEADS:\n",
      "2.654691219329834 \n",
      "8.07150936126709 Times The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  A Break in the Search for the Origin of Complex Life\n",
      "HEADS:\n",
      "5.642500400543213 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Obama s Ingenious Mention of Atticus Finch\n",
      "HEADS:\n",
      "5.055740594863892 The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Donald Trump Meets and Assails the Press\n",
      "HEADS:\n",
      "2.623927593231201 \n",
      "5.386254549026489 York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Trump I Think Hacking Was Russian\n",
      "HEADS:\n",
      "2.5899767875671387 \n",
      "7.812723159790039 The The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Seth Meyers Questions Kellyanne Conway and the Politics of Late Night\n",
      "HEADS:\n",
      "2.474364995956421 \n",
      "5.06833553314209 Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Trump Administration s Conflicts of Interest A Crib Sheet\n",
      "HEADS:\n",
      "2.8561251163482666 \n",
      "13.03275442123413 The Trump York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Longstanding Crisis Facing Tribal Schools\n",
      "HEADS:\n",
      "3.039573907852173 \n",
      "7.925042390823364 and\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Atlantic Daily Loose Ends and Legacy\n",
      "HEADS:\n",
      "2.4976232051849365 \n",
      "5.029552936553955 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  A Pledge for More of the Same at the Pentagon\n",
      "HEADS:\n",
      "2.7099554538726807 \n",
      "5.452836513519287 The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Atlantic Politics Policy Daily Obama Surprises Biden With the Medal of Freedom\n",
      "HEADS:\n",
      "2.521583080291748 \n",
      "5.0459489822387695 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Contradictions of Ben Carson s Vision for American Housing\n",
      "HEADS:\n",
      "2.711766004562378 \n",
      "10.902624607086182 York New The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Baltimore Police Agree to Stop Abusing Their Power\n",
      "HEADS:\n",
      "2.587904453277588 \n",
      "14.68045425415039 Times Times s New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Donald Trump Says 96 Million Are Looking for Work\n",
      "HEADS:\n",
      "2.70086669921875 \n",
      "5.509343385696411 Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  A One Stop Shop for the Alt Right\n",
      "HEADS:\n",
      "Failed to generate headlines.\n",
      "HEAD:  Rich Students Go to Graduate School to Get Richer\n",
      "HEADS:\n",
      "2.54854154586792 \n",
      "7.683845520019531 The New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  There Are No Happy Endings in A Series of Unfortunate Events\n",
      "HEADS:\n",
      "2.6669564247131348 \n",
      "10.971849918365479 York York New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  What Is Marine Le Pen Doing at Trump Tower\n",
      "HEADS:\n",
      "13.813157558441162 The unannounced^ The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Bacteria Send Electrical Pulses a Recruitment Ads\n",
      "HEADS:\n",
      "120.33580017089844 Title Should taping collided admits terrified camera Justices thoroughly Tests Emails Abortion\n",
      "146.24152755737305 Judge Trapped Ethics panda L james Employees Natural version Jobs Fray Microcephaly Ideas <0>^ 80\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Why Killer Whales and Humans Go Through Menopause\n",
      "HEADS:\n",
      "6.461507797241211 New\n",
      "Failed to generate headlines.\n",
      "HEAD:  On The Tonight Show Michelle Obama Cements a Legacy of Empathy\n",
      "HEADS:\n",
      "2.58069109916687 \n",
      "5.271833658218384 Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  What Happens If You Stick Your Head in a Particle Accelerator\n",
      "HEADS:\n",
      "50.60537576675415 Meeting New Voter a Times The Up The\n",
      "Failed to generate headlines.\n",
      "HEAD:  How Swimming Classes Became an Integration Issue in Switzerland\n",
      "HEADS:\n",
      "2.5629684925079346 \n",
      "5.237064838409424 Times\n",
      "Failed to generate headlines.\n",
      "HEAD:  Resistance to the Antibiotic of Last Resort Is Silently Spreading\n",
      "HEADS:\n",
      "198.44681692123413 Fret salt papers bent casting Nikki t Free Commerce Senators Amendment New In of Hollywood of American Evening Support Syrian The Plans Era <0>^ Times\n",
      "Cumulative 1-gram: 0.041667\n",
      "Cumulative 2-gram: 0.204124\n",
      "Cumulative 3-gram: 0.350373\n",
      "Cumulative 4-gram: 0.451801\n",
      "HEAD:  The Perfectly Normal Ways Trump Can Enrich Himself a President\n",
      "HEADS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5625035762786865 \n",
      "5.245250701904297 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Hermit Who Inadvertently Shaped Climate Change Science\n",
      "HEADS:\n",
      "2.929570436477661 \n",
      "5.9648051261901855 York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Trump Thanks L L\n",
      "HEADS:\n",
      "13.111138820648193 New The Times The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Atlantic Daily Insight and Oversight\n",
      "HEADS:\n",
      "Failed to generate headlines.\n",
      "HEAD:  The Atlantic Politics Policy Daily Survey Says Housing Policy\n",
      "HEADS:\n",
      "10.191288709640503 York The New\n",
      "Cumulative 1-gram: 0.045112\n",
      "Cumulative 2-gram: 0.078136\n",
      "Cumulative 3-gram: 0.094181\n",
      "Cumulative 4-gram: 0.102833\n",
      "HEAD:  What the Investigation Into the Chicago Police Department Found\n",
      "HEADS:\n",
      "2.4993014335632324 \n",
      "5.044368743896484 The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Transnational Trolley and Doughnuts in Juarez The Week in Global Affairs Writing\n",
      "HEADS:\n",
      "218.9820327758789 flower Seeks Durst Democrats contraception paid mecca albeit tiny 82 Worries cheering Duterte Dead Oakley a to services Adds of abruptly in Donald Her National\n",
      "219.85660457611084 percent exxon sharon tile Ruling Fox automatically profane dominance puzzle to Briefing Patient Slipping <2>^ Nearly Apple Between Lets Cute Settlements population Times British The\n",
      "Cumulative 1-gram: 0.041667\n",
      "Cumulative 2-gram: 0.204124\n",
      "Cumulative 3-gram: 0.350373\n",
      "Cumulative 4-gram: 0.451801\n",
      "HEAD:  The Trump Dossier and the Making of Intelligence\n",
      "HEADS:\n",
      "2.838771343231201 \n",
      "5.688542127609253 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Waking Up to Coachella s Conservative Tinge\n",
      "HEADS:\n",
      "5.94284200668335 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Ranked The World s Most Unusual Places to Hold Political Meetings\n",
      "HEADS:\n",
      "2.489128589630127 \n",
      "6.719793796539307 of\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  A Fight Over Ethics in the Age of Trump\n",
      "HEADS:\n",
      "2.5686588287353516 \n",
      "5.252424240112305 York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Obamacare Repeal Moves a Step Closer to Reality\n",
      "HEADS:\n",
      "6.213979959487915 York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  How Victoria Aims to Connect With Young Women\n",
      "HEADS:\n",
      "2.7065718173980713 \n",
      "5.486056804656982 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Betsy DeVos s Accountability Problem\n",
      "HEADS:\n",
      "29.871073246002197 Syria Offer Simmons\n",
      "36.407126903533936 Lost Win path York\n",
      "91.41217684745789 contention Links Ban York York New New superstar Times The New E Times Order New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Sessions Exaggerated His Record on Gun Prosecutions\n",
      "HEADS:\n",
      "2.6372835636138916 \n",
      "5.382070779800415 Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Why Elections Are Now Classified a Critical Infrastructure\n",
      "HEADS:\n",
      "2.484596014022827 \n",
      "5.090383768081665 York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  20th Century Women Is an Ode to Female Resilience\n",
      "HEADS:\n",
      "2.594092607498169 \n",
      "7.971704006195068 York York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  From Graduate School to Greek Life This Week s Top 7 Education Stories\n",
      "HEADS:\n",
      "2.6483585834503174 \n",
      "10.724185705184937 Times New York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Breitbart Alumni Launch Populist Nationalist Group\n",
      "HEADS:\n",
      "17.63952946662903 House The The\n",
      "41.69905686378479 New Obama The York Times New Again York York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Why Conservative Politicians May Be More Attractive Than Liberal Ones\n",
      "HEADS:\n",
      "2.4855175018310547 \n",
      "5.007351875305176 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Black ish s Lemons Is Art for the Age of Trump\n",
      "HEADS:\n",
      "42.24940061569214 Aleppo Day Long Final Race\n",
      "59.878769636154175 Against Racial China Ties a The for Trump Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  What Weird Obama Dreams Say About the President s Legacy\n",
      "HEADS:\n",
      "2.8612895011901855 \n",
      "5.825559854507446 Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Why Is Obama Expanding Surveillance Powers Right Before He Leaves Office\n",
      "HEADS:\n",
      "2.5378806591033936 \n",
      "5.106247901916504 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  In Switzerland You Can Be Denied Citizenship for Being Too Annoying\n",
      "HEADS:\n",
      "109.64258909225464 execute Podcast palestinians Wrongfully Bad Revives Queens Confronts Writer Senators product\n",
      "176.90574741363525 sydney Against Lied Administration billionaire Vision Records U Trade Win For Rules on Day the Resigns Profitable Mob Obama but\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  SpaceX Sticks the Landing in Triumphant Return\n",
      "HEADS:\n",
      "2.63252329826355 \n",
      "5.232584238052368 Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Emo Nostalgia and Obama Lit The Week in Pop Culture Writing\n",
      "HEADS:\n",
      "2.5552515983581543 \n",
      "5.151100397109985 The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Can Evangelicals Help Trump Thaw Relations With Russia\n",
      "HEADS:\n",
      "13.006104230880737 The York New New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Former Intel Chief Community Caught Between Scylla and Charybdis on Trump Dossier\n",
      "HEADS:\n",
      "2.838771343231201 \n",
      "7.376190662384033 in\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Why Europe Is Worried About Donald Trump s Latest Remarks\n",
      "HEADS:\n",
      "3.086899995803833 \n",
      "13.743679761886597 of Times The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Toni Erdmann Is a Comedy Experience Unlike Any Other\n",
      "HEADS:\n",
      "126.14816808700562 Try Opens Visit partner colin Should considers Farewell on Trans 5 Times Risk in\n",
      "196.76654529571533 skier Secret liquid Turkish Call recall California Support Faces For Calls Millions a Spending to Hidden Review York It Your York but Up Bankers York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  How Mass Incarceration Pushes Black Children Further Behind in School\n",
      "HEADS:\n",
      "3.028566360473633 \n",
      "6.083937168121338 The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  U S\n",
      "HEADS:\n",
      "13.008883953094482 Possible\n",
      "20.853161334991455 Red Philippines\n",
      "35.977459192276 Clinton Leaves P York With\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  These Pro Lifers Are Headed to the Women s March on Washington\n",
      "HEADS:\n",
      "2.6119635105133057 \n",
      "8.023444890975952 York York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Flesh Eating Worms Reach Florida s Mainland\n",
      "HEADS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194.189594745636 ronald email Governments adult stimulus fracture Castro View New Your Rain newest Briefing Repair Its NBC Not Fighting York of updated stray^ ISIS\n",
      "203.13349437713623 Say Nunes Emotional uproar prospect Troubled adjacent York With News Times Year News Kelly <1>^ Get This amazing 11 Goes Support The House Board\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Why More Writers Should Talk About Money\n",
      "HEADS:\n",
      "202.80805826187134 Facing devout special strain warship Delay U Shoe Speech Gun Reports for Minister and Theaters Gold Soften Times The Gunman Rio Were Times Panama York\n",
      "212.91595029830933 chocolate Crimes l Make pseudonym State Affleck Gawker the Day Nuclear in So Finds John Campaign Editor youngster <0>^ England Syria Pacific Dallas a mathematician\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Americans Can Soon Buy Groceries Online With Food Stamps\n",
      "HEADS:\n",
      "2.5950353145599365 \n",
      "5.339914321899414 New\n",
      "Failed to generate headlines.\n",
      "HEAD:  Seeking an Escape From Trump s America\n",
      "HEADS:\n",
      "6.1408655643463135 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Will Mike Pence Be Trump s Bridge to Democrats\n",
      "HEADS:\n",
      "Failed to generate headlines.\n",
      "HEAD:  How Cash Bail Keeps the Poor in Jail\n",
      "HEADS:\n",
      "2.819216251373291 \n",
      "5.650766134262085 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Can James Mattis Protect Trump From Hubris\n",
      "HEADS:\n",
      "2.70053768157959 \n",
      "18.760481595993042 New Times The The Times York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Shadow Network of Anti Vax Doctors\n",
      "HEADS:\n",
      "2.6688783168792725 \n",
      "5.32596492767334 York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Obama and the Limits of Fact Based Foreign Policy\n",
      "HEADS:\n",
      "61.91923379898071 Grandfather Warren Theaters Daylight In Yemen\n",
      "144.64682960510254 Amazon debating Nafta bin Fades Policy Numbers Survivor Trump Jong After California York Your S Itself to\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Nintendo Wants Players to Look at Each Other Again\n",
      "HEADS:\n",
      "14.226381301879883 Two New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Two Cases Could Limit or Enhance Trump s Ability to Engage in Mass Deportations\n",
      "HEADS:\n",
      "2.714047908782959 \n",
      "5.5381083488464355 York\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  What Betsy DeVos Did and Didn t Reveal About Her Education Priorities\n",
      "HEADS:\n",
      "57.215521812438965 Bridge tibetan <0>^ Health York a Sports The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Atlantic Daily Postures and Policy\n",
      "HEADS:\n",
      "2.539344072341919 \n",
      "5.108316659927368 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  President Obama s Last Clemency Push\n",
      "HEADS:\n",
      "2.655061721801758 \n",
      "5.2291176319122314 The\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Trump s Interests v America s Vornado Edition\n",
      "HEADS:\n",
      "2.8841941356658936 \n",
      "5.871732234954834 Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  A Defamation Suit Against the President Elect\n",
      "HEADS:\n",
      "Failed to generate headlines.\n",
      "HEAD:  The Trump Administration s Conflicts of Interest Scott Pruitt Edition\n",
      "HEADS:\n",
      "5.290916681289673 Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Will States Take Up the Mantle of Worker Protection\n",
      "HEADS:\n",
      "5.389454126358032 New\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The New Celebrity Apprentice and the Scourge of Faux Inclusivity\n",
      "HEADS:\n",
      "12.507549285888672 a s\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  Will Hollywood Learn From Hidden Figures s Success\n",
      "HEADS:\n",
      "42.74340772628784 Cooperation jonas Bright President\n",
      "89.11089181900024 tissue undetected expertly My First You the His Life Apology\n",
      "113.02321481704712 The Era Right Removed Syria and Who Africa Transgender York Holds Blamed York Immigration\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "HEAD:  The Atlantic Daily Presidential Pressure\n",
      "HEADS:\n",
      "2.4976232051849365 \n",
      "5.130254745483398 Times\n",
      "Cumulative 1-gram: 0.000000\n",
      "Cumulative 2-gram: 0.000000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Read from eval pickle file\n",
    "import pickle\n",
    "import csv\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "with open('pickles/eval_100.pickle', 'rb') as fp:\n",
    "    heads, desc, keywords = pickle.load(fp) # keywords are not used in this project\n",
    "\n",
    "with open('5k-2-eval.csv', 'w') as fd:\n",
    "    writer = csv.writer(fd)\n",
    "    writer.writerow(['Reference', 'Description', 'Generated', 'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4'])\n",
    "\n",
    "    for i in range(len(heads)):\n",
    "        Y = heads[i][0]\n",
    "        X = desc[i][0]\n",
    "\n",
    "        print('HEAD: ', Y)\n",
    "        try:\n",
    "            samples = gensamples(X=X, skips=2, batch_size=batch_size, k=10, temperature=1.)\n",
    "            reference = [Y.split()]\n",
    "\n",
    "            headline = samples[0][0][len(samples[0][1]):]\n",
    "\n",
    "            candidate = [idx2word[w] for w in headline]\n",
    "            candidate = candidate[:len(candidate)-1]\n",
    "            head_str = ' '.join(idx2word[w] for w in headline[:len(headline)-1])\n",
    "\n",
    "            # BLEU score\n",
    "            bleu1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "            bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "            bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))\n",
    "            bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "            print('Cumulative 1-gram: %f' % bleu1)\n",
    "            print('Cumulative 2-gram: %f' % bleu2)\n",
    "            print('Cumulative 3-gram: %f' % bleu3)\n",
    "            print('Cumulative 4-gram: %f' % bleu4)\n",
    "\n",
    "            writer.writerow([Y, X, head_str, bleu1, bleu2, bleu3, bleu4])\n",
    "        except:\n",
    "            print('Failed to generate headlines.')\n",
    "            writer.writerow([Y, X, 'N/A', 0, 0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2713,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-2713-c246e9b1973f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2713-c246e9b1973f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    X = \"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.     During his first press conference in several months,   Donald Trump vehemently denied Tuesdays reports alleging that Russia had compromising information about him, calling it fake news.  Trump also announced that his sons will take over the Trump Organization once he becomes president, a move ethics experts say doesnt satisfy    concerns and Trump again refused to release his tax returns, saying the only ones who care about my tax returns are reporters.  Representative John Lewis and Senator Corey Booker testified against Senator Jeff Sessions during the second day of his Senate confirmation hearing to serve as U. S. attorney general. Rex Tillerson, Trumps choice for secretary of state, was critical of Russia during his hearing, saying the country poses a danger to the United States and transportation secretary nominee Elaine Chao said she hopes to unleash the potential of private investment for federal infrastructure projects. Seek the Truth: BuzzFeeds decision to publish a dossier full of serious accusations against   Donald Trump on Tuesday raised serious questions, writes David A. Graham, one of which concerns the journalism ethics of publishing a document full of unverified claims. A Plan in Jeopardy?: Republicans are facing a number of obstacles in their quest to repeal the Affordable Care Act, such as finding a suitable alternative that wont leave millions of Americans without insurance. Still, the GOP is moving forward with its plan   for now. (Russell Berman) Divided We Fall: Trumps critics will need to come together to minimize the damage they fear he will impose, argues Conor Friedersdorf. Yet large swaths of the right and left, including extremely thoughtful,   observers of the American scene, are behaving as if such cooperation is impossible.  Why? Follow stories throughout the day with our Politics  Policy portal. More on Trumps Business Plans: The   says he will turn all business operations over to his two sons once he assumes the presidency and wont enter into any new deals with international partners. But ethics experts say Trump is still facing a constitutional crisis.  (Maggie Haberman, Julie Hirschfeld Davis, and Eric Lipton, The New York Times) On the Defense: The cyber hack on the Democratic Party was instigated by an elite unit of hackers linked to the Russian military intelligence service, known as the GRU, and its targets span the globe and parallel the interests of the Russian state.  The party never stood a chance against them. (Tim Johnson, McClatchy DC) A Look Inside: Marie Claire spoke with former skin chicks, female members of the white supremacist movement. While the cause is   feelings of rage and a desire for community may be contributing to a growing number of women joining the movement. (Kate Storey) Thanks, Obama: Although reviews of his presidency will be mixed, writes Ezra Klein, Americans will miss several things about Barack Obama, namely his decency. His   administration. The seriousness with which he approached his job. The faith he had in the American political system, and in Americans.  (Vox) Solving a Mystery: Raheel Siddiqui, a Muslim Marine recruit, fell to his death during basic training in Parris Island, South Carolina, in March 2016. The Marine Corps ruled the death a suicide, but Siddiquis family thinks differently. Did a night of hazing go wrong? (Alex French, Esquire)   Before and After: Serving as commander in chief of the United States has taken a visible toll on President Obama. Click on these images to see how the president has aged over the past eight years. (Alan Taylor, The Atlantic) Senate confirmation hearings for   Trumps Cabinet nominees began on Tuesday. Whats the one question youd be afraid to answer honestly at your own confirmation hearing? Send your answers to hello@theatlantic. com, and our favorites will be featured in Fridays Politics  Policy Daily.   by Elaine Godfrey (@elainejgodfrey) and Candice Norwood (@cjnorwoodwrites)\"\"\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "X = \"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.     During his first press conference in several months,   Donald Trump vehemently denied Tuesdays reports alleging that Russia had compromising information about him, calling it fake news.  Trump also announced that his sons will take over the Trump Organization once he becomes president, a move ethics experts say doesnt satisfy    concerns and Trump again refused to release his tax returns, saying the only ones who care about my tax returns are reporters.  Representative John Lewis and Senator Corey Booker testified against Senator Jeff Sessions during the second day of his Senate confirmation hearing to serve as U. S. attorney general. Rex Tillerson, Trumps choice for secretary of state, was critical of Russia during his hearing, saying the country poses a danger to the United States and transportation secretary nominee Elaine Chao said she hopes to unleash the potential of private investment for federal infrastructure projects. Seek the Truth: BuzzFeeds decision to publish a dossier full of serious accusations against   Donald Trump on Tuesday raised serious questions, writes David A. Graham, one of which concerns the journalism ethics of publishing a document full of unverified claims. A Plan in Jeopardy?: Republicans are facing a number of obstacles in their quest to repeal the Affordable Care Act, such as finding a suitable alternative that wont leave millions of Americans without insurance. Still, the GOP is moving forward with its plan   for now. (Russell Berman) Divided We Fall: Trumps critics will need to come together to minimize the damage they fear he will impose, argues Conor Friedersdorf. Yet large swaths of the right and left, including extremely thoughtful,   observers of the American scene, are behaving as if such cooperation is impossible.  Why? Follow stories throughout the day with our Politics  Policy portal. More on Trumps Business Plans: The   says he will turn all business operations over to his two sons once he assumes the presidency and wont enter into any new deals with international partners. But ethics experts say Trump is still facing a constitutional crisis.  (Maggie Haberman, Julie Hirschfeld Davis, and Eric Lipton, The New York Times) On the Defense: The cyber hack on the Democratic Party was instigated by an elite unit of hackers linked to the Russian military intelligence service, known as the GRU, and its targets span the globe and parallel the interests of the Russian state.  The party never stood a chance against them. (Tim Johnson, McClatchy DC) A Look Inside: Marie Claire spoke with former skin chicks, female members of the white supremacist movement. While the cause is   feelings of rage and a desire for community may be contributing to a growing number of women joining the movement. (Kate Storey) Thanks, Obama: Although reviews of his presidency will be mixed, writes Ezra Klein, Americans will miss several things about Barack Obama, namely his decency. His   administration. The seriousness with which he approached his job. The faith he had in the American political system, and in Americans.  (Vox) Solving a Mystery: Raheel Siddiqui, a Muslim Marine recruit, fell to his death during basic training in Parris Island, South Carolina, in March 2016. The Marine Corps ruled the death a suicide, but Siddiquis family thinks differently. Did a night of hazing go wrong? (Alex French, Esquire)   Before and After: Serving as commander in chief of the United States has taken a visible toll on President Obama. Click on these images to see how the president has aged over the past eight years. (Alan Taylor, The Atlantic) Senate confirmation hearings for   Trumps Cabinet nominees began on Tuesday. Whats the one question youd be afraid to answer honestly at your own confirmation hearing? Send your answers to hello@theatlantic. com, and our favorites will be featured in Fridays Politics  Policy Daily.   by Elaine Godfrey (@elainejgodfrey) and Candice Norwood (@cjnorwoodwrites)\"\"\n",
    "Y = \"The Atlantic  Politics & Policy Daily: Back-to-Back Sessions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gensamples(X=X, skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \"18 Cake GIFs That 'll Make You Moist\"\n",
    "Y = \"Is it 350degF^ in here or is it just me ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADS:\n",
      "5.842418670654297 enraged playing playing\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "100100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1895-29be5167ca40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1890-8abe4e54a262>\u001b[0m in \u001b[0;36mgensamples\u001b[0;34m(X, X_test, Y_test, avoid, avoid_score, skips, k, batch_size, short, temperature, use_unk)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0meos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mcode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 100100"
     ]
    }
   ],
   "source": [
    "samples = gensamples(X, skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1896,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \"President Barack Obama 's re-election campaign is fundraising off of comments on Obama 's birth certificate by Mitt Romney 's son Matt .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADS:\n",
      "15.432276219129562 Report subsidiary for s May 13 in in May 13\n",
      "20.45819543302059 Report subsidiary for s May 13 in in May 13 in Obama Agent\n",
      "22.169368356466293 Report subsidiary for s May 13 in in May 13 State\n",
      "23.624240252189338 Report subsidiary for s May 13 in in May 13 in Obama forty Report Breitbart\n",
      "25.491706654429436 Report subsidiary for s May 13 in in May 13 in Obama eligible troubling stepped\n",
      "27.66153925814433 Report subsidiary for s May 13 in in May 13 in Obama forty Report retake memoir memoir Breitbart\n",
      "28.18587867100723 Report subsidiary for s May 13 in in May 13 in Obama eligible troubling stepped Breitbart\n",
      "29.74173215031624 Report subsidiary for s May 13 in in May 13 in Obama forty Report retake memoir memoir\n",
      "32.93222215771675 Report subsidiary for s May 13 in in May 13 in Obama forty Report retake memoir memoir Down\n",
      "39.950773272787046 Report subsidiary for s May 13 in in May 13 in Obama forty Report retake memoir memoir Down athlete Breitbart\n"
     ]
    }
   ],
   "source": [
    "gensamples(X, skips=2, batch_size=batch_size, k=10, temperature=1, use_unk=True, short=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1898,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \"What have you been listening to this year ? If you want to find out using cold , hard evidence , then Spotify 's new Year in Music tool will tell you .\"\n",
    "Y = \"Spotify Will Make You Smarter for Your App\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADS:\n",
      "21.190365344285965 Donald Trump s election to first state state\n"
     ]
    }
   ],
   "source": [
    "samples = gensamples(X, skips=2, batch_size=batch_size, k=10, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1900,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = samples[0][0][len(samples[0][1]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1901,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Donald Trump s election to first state state ~'"
      ]
     },
     "execution_count": 1901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(idx2word[w] for w in headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1902,
   "metadata": {},
   "outputs": [],
   "source": [
    "avoid = headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADS:\n",
      "15.060724568367004 Donald Trump rent behalf illinois presidential process\n"
     ]
    }
   ],
   "source": [
    "samples = gensamples(X, avoid=avoid, avoid_score=.1, skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1904,
   "metadata": {},
   "outputs": [],
   "source": [
    "avoid = samples[0][0][len(samples[0][1]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADS:\n",
      "16.590157836675644 Donald Trump rent North finance presidential election\n"
     ]
    }
   ],
   "source": [
    "samples = gensamples(X, avoid=avoid, avoid_score=.1, skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 1906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1907,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsimple_context(X, mask, n=activation_rnn_size, maxlend=maxlend, maxlenh=maxlenh):\n",
    "    desc, head = X[:,:maxlend], X[:,maxlend:]\n",
    "    head_activations, head_words = head[:,:,:n], head[:,:,n:]\n",
    "    desc_activations, desc_words = desc[:,:,:n], desc[:,:,n:]\n",
    "    \n",
    "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
    "    # activation for every head word and every desc word\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=([2],[2]))\n",
    "    # make sure we dont use description words that are masked out\n",
    "    assert mask.ndim == 2\n",
    "    activation_energies = K.switch(mask[:, None, :maxlend], activation_energies, -1e20)\n",
    "    \n",
    "    # for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights,(-1,maxlenh,maxlend))\n",
    "\n",
    "    return activation_weights\n",
    "\n",
    "\n",
    "class WSimpleContext(Lambda):\n",
    "    def __init__(self):\n",
    "        super(WSimpleContext, self).__init__(wsimple_context)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return input_mask[:, maxlend:]\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        nb_samples = input_shape[0]\n",
    "        n = lambda_dim\n",
    "        return (nb_samples, maxlenh, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1908,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmodel = Sequential()\n",
    "wmodel.add(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1909,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmodel.add(Lambda(wsimple_context,\n",
    "                 mask = lambda inputs, mask: mask[:,maxlend:],\n",
    "                 output_shape = lambda input_shape: (input_shape[0], maxlenh, maxlend)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1910,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1911,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1912,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = \"Representatives of the groups depicted in The Revenant^ spoke with BuzzFeed News about the actor 's Golden Globes speech calling on listeners to `` protect ... indigenous lands . ''\"\n",
    "# Y = \"Native American Groups Officially Respond To Leonardo DiCaprio 's Call To Action\"\n",
    "X = \"Ivanka Trump, special assistant to the president, told Fox News on Monday: I try to stay out of politics.  Trump was speaking to Fox  Friends, the morning show which this weekend broadcast an interview with Donald Trump and first lady Melania Trump. I try to stay out of politics, Ivanka Trump said in answer to a question about her fathers use of Twitter to bypass most normal channels of presidential communication. His political instincts are phenomenal. He did something that no one could have imagined hed be able to accomplish.  I feel blessed just being part of the ride from day one and before. But he did something pretty remarkable. But I dont profess to be a political savant.  Donald Trump won the 2016 election in the electoral college, after a campaign marked by bitter partisan rancour and interference by Russian actors working, according to a Washington Post report last week, at the direct instruction of Vladimir Putin to help Trump beat Hillary Clinton. Clinton won the popular vote by more than 2. 5 million ballots. Ivanka Trump became a trusted lieutenant to her father, delivering a   convention speech in Cleveland in July and moving to Washington in January with her husband, Jared Kushner, who is one of the presidents closest advisers. Her White House position is unpaid. Though Trump claims to stay out of politics she has been a familiar surrogate for her father in the media and on the world stage. Earlier in June, she told Fox  Friends: Were really focused on why the American people elected Donald Trump as their president and implementing that plan.  Amidst congressional and FBI investigations into links between Trump aides and Russia and the reported investigation of her father for possible obstruction of justice related to those probes, she said then: It is hard, and theres a level of viciousness that I was not expecting. I was not expecting the intensity of this experience, but this isnt supposed to be easy.  Asked in the interview broadcast on Monday if she ever disagreed with her father     who has, for example, pursued policies on climate change, pulling out of the Paris agreement, that might be thought anathema to a registered New York Democrat, which she until recently was     the first daughter said: So naturally, there are areas where there is disagreement.  Climate change, with womens rights, is part of Ivanka Trumps White House brief. Were two different human beings, she continued. I think its normal to not have 100% aligned viewpoints on every issue. I dont think anyone operates like that with a parent, or within the context of an administration. And I think that all different viewpoints being at the table is a positive thing. And I think one of the things that, in this country we dont have enough of, is dialogue.  \"\n",
    "Y = \"Ivanka Trump says I try to stay out of politics despite assisting at White House\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1927,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADS:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (944,) and (25,25) not aligned: 944 (dim 0) != 25 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1927-29be5167ca40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1890-8abe4e54a262>\u001b[0m in \u001b[0;36mgensamples\u001b[0;34m(X, X_test, Y_test, avoid, avoid_score, skips, k, batch_size, short, temperature, use_unk)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mfold_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         sample, score = beamsearch(predict=keras_rnn_predict, start=fold_start, avoid=avoid, avoid_score=avoid_score,\n\u001b[0;32m---> 40\u001b[0;31m                                    k=k, temperature=temperature, use_unk=use_unk)\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaxlend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0meos\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1885-4cdc060b4957>\u001b[0m in \u001b[0;36mbeamsearch\u001b[0;34m(predict, start, avoid, avoid_score, k, maxsample, use_unk, oov, empty, eos, temperature)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlive_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# for every possible live sample calc prob for every possible label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlive_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1887-2fe8e38b2c39>\u001b[0m in \u001b[0;36mkeras_rnn_predict\u001b[0;34m(samples, empty, model, maxlen)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput2probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmaxlend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_length\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1887-2fe8e38b2c39>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput2probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmaxlend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_length\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1878-139758ea8e6b>\u001b[0m in \u001b[0;36moutput2probs\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# out very own softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moutput2probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (944,) and (25,25) not aligned: 944 (dim 0) != 25 (dim 0)"
     ]
    }
   ],
   "source": [
    "samples = gensamples(X, skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1914,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<0>^ she has been a familiar surrogate for her father in the media and on the world <1>^ Earlier in <2>^ she told Fox <3>^ ~ How the staunch for the doj gonna breast ~'"
      ]
     },
     "execution_count": 1915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[w] for w in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1916,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 1916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sequence.pad_sequences([sample], maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25, 25)"
      ]
     },
     "execution_count": 1917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = wmodel.predict(data, verbose=0, batch_size=1)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1918,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 8)"
      ]
     },
     "execution_count": 1918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startd = np.where(data[0,:] != empty)[0][0]\n",
    "try:\n",
    "    lenh = np.where(data[0,maxlend+1:] == eos)[0][0]\n",
    "except:\n",
    "    print('No EOS in data[0,maxlend+1:], using data[0,maxlend+1:].shape[0] instead')\n",
    "    lenh = data[0, maxlend+1:].shape[0]\n",
    "startd, lenh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c30b509e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(np.array(weights[0,:lenh,startd:].flatten()+1), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1920,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def heat(sample,weights,dark=0.3):\n",
    "    weights = (weights - weights.min())/(weights.max() - weights.min() + 1e-4)\n",
    "    html = ''\n",
    "    fmt = ' <span style=\"background-color: #{0:x}{0:x}ff\">{1}</span>'\n",
    "    for t,w in zip(sample,weights):\n",
    "        c = int(256*((1.-dark)*(1.-w)+dark))\n",
    "        html += fmt.format(c,idx2word[t])\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1921,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat(sample, weights[0,-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
